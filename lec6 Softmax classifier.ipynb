{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax classifier\n",
    "\n",
    "본격적인 정규화작업이라 생각 된다.\n",
    "기본적인 로지스틱 회귀분석에서 벡터화를 통해 여러 자료들을 한통으로 분석한다고 봐도 무방하다\n",
    "이전의 로지스틱 회귀분석에서 수많은 자료들을 벡터화 시켜 자료를 넣는 방식이다. 가령 아래는 로지스틱 회귀분석에서 쓰였던 행렬인데\n",
    "$$\\begin{bmatrix} { w }_{ 1 } & { w }_{ 2 } & { w }_{ 3 } \\end{bmatrix}\\times \\begin{bmatrix} { x }_{ 1 } \\\\ { x }_{ 2 } \\\\ { x }_{ 3 } \\end{bmatrix}={ w }_{ 1 }{ x }_{ 1 }+{ w }_{ 2 }{ x }_{ 2 }+{ w }_{ 3 }{ x }_{ 3 }$$\n",
    "softmax에서는 아래와 같이 바꿔 사용하게 된다.\n",
    "$$\\begin{bmatrix}{w}_{a1} & {w}_{a2} & {w}_{a3}\\\\\n",
    "{w}_{b1} & {w}_{b2} & {w}_{b3}\\\\\n",
    "{w}_{c1} & {w}_{c2} & {w}_{c3}\n",
    "\\end{bmatrix}\\times\\begin{bmatrix}{x}_{1}\\\\\n",
    "{x}_{2}\\\\\n",
    "{x}_{3}\n",
    "\\end{bmatrix}=\\left[\\begin{array}{c}\n",
    "{w}_{a1}{x}_{1}+{w}_{a2}{x}_{2}+{w}_{a3}{x}_{3}\\\\\n",
    "{w}_{b1}{x}_{1}+{w}_{b2}{x}_{2}+{w}_{b3}{x}_{3}\\\\\n",
    "{w}_{c1}{x}_{1}+{w}_{c2}{x}_{2}+{w}_{c3}{x}_{3}\n",
    "\\end{array}\\right] $$\n",
    "이전 수업에서 분명 로지스틱 회귀분석을 시그모이드함수로 정규화 하기전에 아래와 같은 가설을 활용했었다.\n",
    "$$ {w}_{a1}{x}_{1}+{w}_{a2}{x}_{2}+{w}_{a3}{x}_{3}=H_{a}(x) $$\n",
    "위의 것을 이용하면 아래와 같이 풀이될수 있다.\n",
    "$$\\left[\\begin{array}{c}\n",
    "{w}_{a1}{x}_{1}+{w}_{a2}{x}_{2}+{w}_{a3}{x}_{3}\\\\\n",
    "{w}_{b1}{x}_{1}+{w}_{b2}{x}_{2}+{w}_{b3}{x}_{3}\\\\\n",
    "{w}_{c1}{x}_{1}+{w}_{c2}{x}_{2}+{w}_{c3}{x}_{3}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "H_{a}(x)\\\\\n",
    "H_{b}(x)\\\\\n",
    "H_{c}(x)\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\overline{y_{a}}\\\\\n",
    "\\overline{y_{b}}\\\\\n",
    "\\overline{y_{c}}\n",
    "\\end{array}\\right]\n",
    " $$\n",
    "여기에서 sotfmax함수를 사용하여 정규화를 하는것이 최종적인 가설이 되는 것이다.\n",
    "아래는 실제 예시를 통하여 소프트맥스함수의 정규화를 보여준다.\n",
    "$$\\left[\\begin{array}{c}\n",
    "\\overline{y_{a}}\\\\\n",
    "\\overline{y_{b}}\\\\\n",
    "\\overline{y_{c}}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "2.0\\\\\n",
    "1.0\\\\\n",
    "0.1\n",
    "\\end{array}\\right]\n",
    " $$\n",
    " 이것을 소프트맥스 함수에 넣으면 아래와 같이 되는데\n",
    " $$s(\\left[\\begin{array}{c}\n",
    "2.0\\\\\n",
    "1.0\\\\\n",
    "0.1\n",
    "\\end{array}\\right])=\\left[\\begin{array}{c}\n",
    "0.7\\\\\n",
    "0.2\\\\\n",
    "0.1\n",
    "\\end{array}\\right]\\rightarrow\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right]\n",
    " $$\n",
    " 보시다시피 정규화 과정을 통하여 확률로 변환이 되었으며, 자연스레 가장 높은 확률을 가진것이 결과가 되는 것이다.\n",
    " \n",
    " ### cost함수\n",
    " 이제 가설을 만들었으니이를 통해 학습을 할수 있도록 cost 함수를 만들고 minimise하는 방법을 알아야 한다.\n",
    " 소프트맥스 함수의 cost함수는 cross-entropy함수를 이용하게 되는데 아래 공식과 같다.\n",
    " $$D(S,L)=\\underset{i}{-\\sum}L_{i}log(S_{i})=\\underset{i}{-\\sum}L_{i}log(\\overline{y_{i}})=\\underset{i}{\\sum\\times-}L_{i}(log(\\overline{y_{i}}))$$\n",
    " 여기를 보면 log를 통하여 격차가 크면 클수록 배로 큰 오차를 보여주게 된다. 그리고 이것을 경사하강법에 넣고 돌림으로서\n",
    " minimise cost를 할 수 있다,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
